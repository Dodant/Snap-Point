{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SnapPoint 신경망"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import utils\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "import PIL.Image as pilimg\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing import image\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = np.load('np/data.npy')\n",
    "data_y = np.load('np/data_label.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2227, 5, 40)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1892"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n=int(2227*0.85)\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(335, 5, 40)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_x[n:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = data_x[:n]\n",
    "train_y = data_y[:n]\n",
    "test_x = data_x[n:]\n",
    "test_y = data_y[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "# Step 1. Data Preprocessing\n",
    "train_x=np.reshape(train_x,(-1,train_x.shape[1],train_x.shape[2],1))\n",
    "test_x=np.reshape(test_x,(-1,test_x.shape[1],test_x.shape[2],1))\n",
    "train_y = utils.to_categorical(train_y,10)\n",
    "test_y = utils.to_categorical(test_y,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1892, 10)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape\n",
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_size = (2, 2)  # size of pooling area for max pooling\n",
    "kernel_size = (3, 3)  # convolution kernel size\n",
    "np.random.seed(1337)\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(Conv2D(8, kernel_size, padding='same', input_shape=(5,40,1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=pool_size))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(16, kernel_size, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=pool_size))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(600))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile (optimizer= 'adam', \n",
    "                loss='binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1513 samples, validate on 379 samples\n",
      "Epoch 1/12\n",
      "1513/1513 - 0s - loss: 0.9996 - accuracy: 0.9141 - val_loss: 3.0438 - val_accuracy: 0.8000\n",
      "Epoch 2/12\n",
      "1513/1513 - 0s - loss: 0.5917 - accuracy: 0.9433 - val_loss: 2.1209 - val_accuracy: 0.8084\n",
      "Epoch 3/12\n",
      "1513/1513 - 0s - loss: 0.4094 - accuracy: 0.9581 - val_loss: 1.0072 - val_accuracy: 0.8712\n",
      "Epoch 4/12\n",
      "1513/1513 - 0s - loss: 0.2875 - accuracy: 0.9634 - val_loss: 0.6112 - val_accuracy: 0.9061\n",
      "Epoch 5/12\n",
      "1513/1513 - 0s - loss: 0.1843 - accuracy: 0.9722 - val_loss: 0.2772 - val_accuracy: 0.9425\n",
      "Epoch 6/12\n",
      "1513/1513 - 0s - loss: 0.1505 - accuracy: 0.9762 - val_loss: 0.2193 - val_accuracy: 0.9525\n",
      "Epoch 7/12\n",
      "1513/1513 - 0s - loss: 0.1172 - accuracy: 0.9779 - val_loss: 0.2689 - val_accuracy: 0.9435\n",
      "Epoch 8/12\n",
      "1513/1513 - 0s - loss: 0.0898 - accuracy: 0.9800 - val_loss: 0.1621 - val_accuracy: 0.9625\n",
      "Epoch 9/12\n",
      "1513/1513 - 0s - loss: 0.0758 - accuracy: 0.9822 - val_loss: 0.2034 - val_accuracy: 0.9504\n",
      "Epoch 10/12\n",
      "1513/1513 - 0s - loss: 0.0690 - accuracy: 0.9818 - val_loss: 0.2090 - val_accuracy: 0.9515\n",
      "Epoch 11/12\n",
      "1513/1513 - 0s - loss: 0.0527 - accuracy: 0.9851 - val_loss: 0.1925 - val_accuracy: 0.9530\n",
      "Epoch 12/12\n",
      "1513/1513 - 0s - loss: 0.0617 - accuracy: 0.9832 - val_loss: 0.1659 - val_accuracy: 0.9578\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1aa329a6e88>"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x, train_y, batch_size=128, epochs=12, verbose=2, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "335/1 - 0s - loss: 0.1276 - accuracy: 0.9534\n",
      "test_loss =  0.19244698857638373 test_acc =  0.9534328\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_x, test_y, verbose=2)\n",
    "print('test_loss = ', test_loss, 'test_acc = ', test_acc)\n",
    "\n",
    "predictions = model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('SnapPoint.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import librosa\n",
    "def cutting_sound(filepath):\n",
    "    batch_size = 0\n",
    "    data_height = 5\n",
    "    data_width = 40\n",
    "    train = np.zeros((batch_size, data_height, data_width))\n",
    "\n",
    "    file_list = glob.glob(filepath + '*.wav')\n",
    "    for file_name in file_list:\n",
    "        y, sr = librosa.load(file_name, sr=20000)\n",
    "        y_mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40, hop_length=500)\n",
    "        _, beat_frames = librosa.beat.beat_track(y=y, sr=sr, hop_length=500)\n",
    "        for frame_number in beat_frames:\n",
    "            listSound = np.expand_dims(y_mfcc.T[frame_number-1:frame_number+4],axis=0)\n",
    "            train = np.concatenate((train,listSound),axis=0)\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample=cutting_sound(\"./test/\")\n",
    "sample=np.reshape(sample,(-1,sample.shape[1],sample.shape[2],1))\n",
    "predictions = model.predict(sample)\n",
    "sample_y = utils.to_categorical(np.ones(8),10)\n",
    "sample_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great  1\n",
      "great  2\n",
      "great  3\n",
      "great  4\n",
      "great  5\n",
      "great  6\n",
      "great  7\n",
      "great  8\n"
     ]
    }
   ],
   "source": [
    "cnt=0\n",
    "for i in predictions:\n",
    "    if np.argmax(i) == 1:\n",
    "        print(\"great \", cnt+1)\n",
    "    else:\n",
    "        print(\"failed\")\n",
    "    cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.67478658e-02, 9.82653916e-01, 8.57273044e-05, 5.33437851e-05,\n",
       "        8.33375889e-05, 3.09740535e-05, 5.13193008e-05, 2.78371572e-05,\n",
       "        6.94553773e-06, 2.58727203e-04],\n",
       "       [3.99518646e-02, 9.53156352e-01, 6.98683027e-04, 9.01997846e-04,\n",
       "        1.22854800e-03, 9.12750431e-04, 5.59262873e-04, 7.77408713e-04,\n",
       "        1.52117093e-04, 1.66110659e-03],\n",
       "       [1.26863718e-01, 8.54404211e-01, 1.46091043e-03, 3.98336444e-03,\n",
       "        3.80505878e-03, 8.16942484e-04, 2.38740584e-03, 1.78354804e-03,\n",
       "        3.59150261e-04, 4.13575396e-03],\n",
       "       [7.32165277e-02, 9.18803096e-01, 9.09890921e-04, 1.57846848e-03,\n",
       "        1.18899497e-03, 6.57666998e-04, 1.00478681e-03, 8.83436122e-04,\n",
       "        2.54236511e-04, 1.50281773e-03],\n",
       "       [1.50058803e-03, 9.98321116e-01, 3.01190230e-05, 3.73838084e-05,\n",
       "        3.61244638e-05, 5.80593951e-06, 2.88122501e-05, 1.09512284e-05,\n",
       "        1.53256929e-06, 2.75999701e-05],\n",
       "       [2.63508886e-01, 5.65124154e-01, 1.76612996e-02, 2.87296921e-02,\n",
       "        3.43770124e-02, 1.88971348e-02, 1.85679253e-02, 1.91046838e-02,\n",
       "        9.49223153e-03, 2.45369878e-02],\n",
       "       [1.19986078e-02, 9.87946212e-01, 4.84762995e-06, 1.20090235e-05,\n",
       "        4.08562300e-06, 5.73754448e-07, 1.05471199e-05, 2.23542361e-06,\n",
       "        6.47038917e-07, 2.01483235e-05],\n",
       "       [6.74616918e-02, 9.05366898e-01, 2.69627711e-03, 3.56784393e-03,\n",
       "        3.92695889e-03, 2.34443671e-03, 3.57850944e-03, 3.63931339e-03,\n",
       "        1.55322009e-03, 5.86486654e-03]], dtype=float32)"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
